{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97c4251",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "## FINM 25000 - 2025\n",
    "\n",
    "### UChicago Financial Mathematics\n",
    "\n",
    "* Mark Hendricks\n",
    "* hendricks@uchicago.edu\n",
    "\n",
    "## HBS Case\n",
    "\n",
    "### *The Harvard Management Company and Inflation-Indexed Bonds*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef92ba4",
   "metadata": {},
   "source": [
    "### Notation\n",
    "(Hidden LaTeX commands)\n",
    "\n",
    "$$\\newcommand{\\mux}{\\tilde{\\boldsymbol{\\mu}}}$$\n",
    "$$\\newcommand{\\wtan}{\\boldsymbol{\\text{w}}^{\\text{tan}}}$$\n",
    "$$\\newcommand{\\wtarg}{\\boldsymbol{\\text{w}}^{\\text{port}}}$$\n",
    "$$\\newcommand{\\mutarg}{\\tilde{\\boldsymbol{\\mu}}^{\\text{port}}}$$\n",
    "$$\\newcommand{\\wEW}{\\boldsymbol{\\text{w}}^{\\text{EW}}}$$\n",
    "$$\\newcommand{\\wRP}{\\boldsymbol{\\text{w}}^{\\text{RP}}}$$\n",
    "$$\\newcommand{\\wREG}{\\boldsymbol{\\text{w}}^{\\text{REG}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb624b4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfdadc",
   "metadata": {},
   "source": [
    "# 1. HMC's Approach\n",
    "\n",
    "**Section 1 is not graded**, and you do not need to submit your answers. But you are encouraged to think about them, and we will discuss them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eafffb",
   "metadata": {},
   "source": [
    "### 1. \n",
    "There are thousands of individual risky assets in which HMC can invest.  Explain why MV optimization across 1,000 securities is infeasible.\n",
    "\n",
    "### 2.\n",
    "Rather than optimize across all securities directly, HMC runs a two-stage optimization.\n",
    "1. They build asset class portfolios with each one optimized over the securities of the specific asset class.  \n",
    "2. HMC combines the asset-class portfolios into one total optimized portfolio.\n",
    "\n",
    "In order for the two-stage optimization to be a good approximation of the full MV-optimization on all assets, what must be true of the partition of securities into asset classes?\n",
    "\n",
    "### 3.\n",
    "Should TIPS form a new asset class or be grouped into one of the other 11 classes?\n",
    "\n",
    "### 4. \n",
    "Why does HMC focus on real returns when analyzing its portfolio allocation? Is this just a matter of scaling, or does using real returns versus nominal returns potentially change the MV solution?\n",
    "\n",
    "### 5.\n",
    "The case discusses the fact that Harvard places bounds on the portfolio allocation rather than implementing whatever numbers come out of the MV optimization problem.\n",
    "\n",
    "How might we adjust the stated optimization problem in the lecture notes to reflect the extra constraints Harvard is using in their bounded solutions given in Exhibits 5 and 6?\n",
    "\n",
    "### 6. \n",
    "Exhibits 5 shows zero allocation to domestic equities and domestic bonds across the entire computed range of targeted returns, (5.75% to 7.25%). Conceptually, why is the constraint binding in all these cases? What would the unconstrained portfolio want to do with those allocations and why?\n",
    "\n",
    "### 7.\n",
    "Exhibit 6 changes the constraints, (tightening them in most cases.) How much deterioration do we see in the mean-variance tradeoff that Harvard achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7adf1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1614003",
   "metadata": {},
   "source": [
    "# 2 Mean-Variance Optimization\n",
    "\n",
    "<i>This section is graded for a good-faith effort by your group. Submit your write-up- along with your supporting code. </i>\n",
    "\n",
    "### Data\n",
    "You will need the file in the github repo, `data/multi_asset_etf_data.xlsx`.\n",
    "- The time-series data gives monthly returns for the 11 asset classes and a short-term Treasury-bill fund return, (\"SHV\",) which we consider as the risk-free rate.\n",
    "- The data is provided in total returns, (in which case you should ignore the SHV column,) as well as excess returns, (where SHV has been subtracted from the other columns.)\n",
    "- These are nominal returns-they are not adjusted for inflation, and in our calculations we are not making any adjustment for inflation.\n",
    "- The exhibit data that comes via Harvard with the case is unnecessary for our analysis.\n",
    "\n",
    "### Model\n",
    "We are going to analyze the problem in terms of **excess** returns.\n",
    "- Thus, you will focus on the `Excess Returns` section of the lecture notes, especially the formulas on slide 50.\n",
    "- Be sure to use the`excess returns` tab of the data.\n",
    "\n",
    "### Format\n",
    "In the questions below, **annualize the statistics** you report.\n",
    "- Annualize the mean of monthly returns with a scaling of 12.\n",
    "- Annualize the volatility of monthly returns with a scaling of $\\sqrt{12}$\n",
    "- The Sharpe Ratio is the mean return divided by the volatility of returns. Accordingly, we can annualize the Sharpe Ratio with a scaling of $\\sqrt{12}$\n",
    "- Note that we are not scaling the raw timeseries data, just the statistics computed from it (mean, vol, Sharpe). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa47172",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "#### Data File\n",
    "* The case does not give time-series data, so this data has been compiled outside of the case, and it intends to represent the main asset classes under consideration via various ETFs. For details on the specific securities/indexes, check the “Info” tab of the data.\n",
    "\n",
    "#### Risk-free rate\n",
    "* In the lecture-note we considered a constant risk-free rate. It is okay that our risk-free rate changes over time, but the assumption is that investors know it’s value one-period ahead of time. Thus, at any given point in time, it is a risk-free rate for the next period. (This is often discussed as the \"bank account\" or \"money market account\" in other settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3962427",
   "metadata": {},
   "source": [
    "### 1. Summary Statistics\n",
    "* Calculate and display the mean and volatility of each asset’s excess return. (Recall we use volatility to refer to standard deviation.)\n",
    "* Which assets have the best and worst Sharpe ratios? Recall that the Sharpe Ratio is simply the ratio of the mean-to-volatility of excess returns:\n",
    "$$\\text{sharpe ratio of investment }i = \\frac{\\mux_i}{\\sigma_i}$$\n",
    "\n",
    "Be sure to annualize all three statss (mean, vol, Sharpe).\n",
    "* mean is scaled by `12`\n",
    "* vol is scaled by `sqrt(12)`\n",
    "* Sharpe is scaled by `sqrt(12)`\n",
    "\n",
    "\n",
    "### 2. Descriptive Analysis\n",
    "* Calculate the correlation matrix of the returns. Which pair has the highest correlation? And the lowest?\n",
    "* How well have TIPS done in our sample? Have they outperformed domestic bonds? Foreign bonds?\n",
    "\n",
    "### 3. The MV frontier.\n",
    "* Compute and display the weights of the tangency portfolios: $\\wtan$.\n",
    "* Does the ranking of weights align with the ranking of Sharpe ratios?\n",
    "* Compute the mean, volatility, and Sharpe ratio for the tangency portfolio corresponding to\n",
    "$\\wtan$.\n",
    "\n",
    "### 4. TIPS\n",
    "Assess how much the tangency portfolio (and performance) change if...\n",
    "* TIPS are dropped completely from the investment set.\n",
    "* The expected excess return to TIPS is adjusted to be 0.0012 higher than what the historic sample shows.\n",
    "\n",
    "Based on the analysis, do TIPS seem to expand the investment opportunity set, implying that Harvard should consider them as a separate asset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8609d16",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59490d8",
   "metadata": {},
   "source": [
    "# 3. Allocations\n",
    "\n",
    "<i>This section is graded for a good-faith effort by your group. Submit your write-up- along with your supporting code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5eb238",
   "metadata": {},
   "source": [
    "* Continue with the same data file as the previous section.\n",
    "\n",
    "* Suppose the investor has a targeted mean excess return (per month) of $\\mutarg$ = 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d46b44c",
   "metadata": {},
   "source": [
    "Build the following portfolios:\n",
    "\n",
    "#### Equally-weighted (EW)\n",
    "Rescale the entire weighting vector to have target mean $\\mutarg$. Thus, the $i$ element of the weight vector is,\n",
    "$$\\wEW_i = \\frac{1}{n}$$\n",
    "\n",
    "#### “Risk-parity” (RP)\n",
    "Risk-parity is a term used in a variety of ways, but here we have in mind setting the weight of the portfolio to be proportional to the inverse of its full-sample variance estimate. Thus, the $i$ element of the weight vector is,\n",
    "$$\\wRP_i = \\frac{1}{\\sigma_i^2}$$\n",
    "\n",
    "#### Mean-Variance (MV)\n",
    "As described in `Section 2`.\n",
    "\n",
    "\n",
    "### Comparing\n",
    "\n",
    "In order to compare all these allocation methods, rescale each weight vector, such that it has targeted mean return of $\\mutarg$.\n",
    "\n",
    "* Calculate the performance of each of these portfolios over the sample.\n",
    "* Report their mean, volatility, and Sharpe ratio. \n",
    "* How does performance compare across allocation methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbb1ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sheets: ['descriptions', 'prices', 'total returns', 'excess returns']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>BWX</th>\n",
       "      <th>DBC</th>\n",
       "      <th>EEM</th>\n",
       "      <th>EFA</th>\n",
       "      <th>HYG</th>\n",
       "      <th>IEF</th>\n",
       "      <th>IYR</th>\n",
       "      <th>PSP</th>\n",
       "      <th>QAI</th>\n",
       "      <th>SPY</th>\n",
       "      <th>TIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-02-28</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.035908</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>-0.001674</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.040556</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.035147</td>\n",
       "      <td>0.007618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>0.008755</td>\n",
       "      <td>0.026909</td>\n",
       "      <td>0.063224</td>\n",
       "      <td>-0.023555</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>-0.001218</td>\n",
       "      <td>-0.010607</td>\n",
       "      <td>0.016995</td>\n",
       "      <td>0.005849</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.012231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-04-30</td>\n",
       "      <td>0.048760</td>\n",
       "      <td>0.045514</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>0.056214</td>\n",
       "      <td>0.015932</td>\n",
       "      <td>0.018369</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.058627</td>\n",
       "      <td>0.018989</td>\n",
       "      <td>0.028917</td>\n",
       "      <td>0.023735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-05-31</td>\n",
       "      <td>-0.012945</td>\n",
       "      <td>-0.051124</td>\n",
       "      <td>-0.028800</td>\n",
       "      <td>-0.021461</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.025654</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>-0.040965</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.010615</td>\n",
       "      <td>0.003259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>-0.042318</td>\n",
       "      <td>-0.009027</td>\n",
       "      <td>-0.011781</td>\n",
       "      <td>-0.005378</td>\n",
       "      <td>-0.004716</td>\n",
       "      <td>-0.030733</td>\n",
       "      <td>-0.042254</td>\n",
       "      <td>-0.010449</td>\n",
       "      <td>-0.016542</td>\n",
       "      <td>0.007876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>-0.002198</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>0.018388</td>\n",
       "      <td>0.044877</td>\n",
       "      <td>0.010472</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.015780</td>\n",
       "      <td>0.064862</td>\n",
       "      <td>0.013104</td>\n",
       "      <td>0.023724</td>\n",
       "      <td>0.010570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.026915</td>\n",
       "      <td>0.007087</td>\n",
       "      <td>0.025382</td>\n",
       "      <td>0.035246</td>\n",
       "      <td>-0.042722</td>\n",
       "      <td>-0.008567</td>\n",
       "      <td>-0.015310</td>\n",
       "      <td>0.018957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.019885</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>-0.013701</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>-0.026225</td>\n",
       "      <td>-0.063669</td>\n",
       "      <td>-0.016079</td>\n",
       "      <td>-0.058562</td>\n",
       "      <td>0.003954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>0.054708</td>\n",
       "      <td>-0.088766</td>\n",
       "      <td>-0.001615</td>\n",
       "      <td>0.033962</td>\n",
       "      <td>-0.001817</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>-0.024503</td>\n",
       "      <td>-0.009856</td>\n",
       "      <td>-0.005544</td>\n",
       "      <td>-0.011659</td>\n",
       "      <td>-0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>-0.007297</td>\n",
       "      <td>0.012373</td>\n",
       "      <td>0.037522</td>\n",
       "      <td>0.045208</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>-0.015094</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.041935</td>\n",
       "      <td>0.016515</td>\n",
       "      <td>0.060147</td>\n",
       "      <td>-0.009017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       BWX       DBC       EEM       EFA       HYG       IEF  \\\n",
       "0   2011-02-28  0.007027  0.042120 -0.000027  0.035908  0.014763 -0.001674   \n",
       "1   2011-03-31  0.008755  0.026909  0.063224 -0.023555  0.000752 -0.001218   \n",
       "2   2011-04-30  0.048760  0.045514  0.027283  0.056214  0.015932  0.018369   \n",
       "3   2011-05-31 -0.012945 -0.051124 -0.028800 -0.021461  0.001933  0.025654   \n",
       "4   2011-06-30  0.000230 -0.042318 -0.009027 -0.011781 -0.005378 -0.004716   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "167 2025-01-31 -0.002198  0.024463  0.018388  0.044877  0.010472  0.003033   \n",
       "168 2025-02-28  0.011376 -0.001250  0.008855  0.026915  0.007087  0.025382   \n",
       "169 2025-03-31  0.007510  0.019885  0.008497 -0.001004 -0.013701  0.000577   \n",
       "170 2025-04-30  0.054708 -0.088766 -0.001615  0.033962 -0.001817  0.007573   \n",
       "171 2025-05-31 -0.007297  0.012373  0.037522  0.045208  0.014835 -0.015094   \n",
       "\n",
       "          IYR       PSP       QAI       SPY       TIP  \n",
       "0    0.045614  0.040556  0.002993  0.035147  0.007618  \n",
       "1   -0.010607  0.016995  0.005849  0.000448  0.012231  \n",
       "2    0.046589  0.058627  0.018989  0.028917  0.023735  \n",
       "3    0.010733 -0.040965  0.000600 -0.010615  0.003259  \n",
       "4   -0.030733 -0.042254 -0.010449 -0.016542  0.007876  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "167  0.015780  0.064862  0.013104  0.023724  0.010570  \n",
       "168  0.035246 -0.042722 -0.008567 -0.015310  0.018957  \n",
       "169 -0.026225 -0.063669 -0.016079 -0.058562  0.003954  \n",
       "170 -0.024503 -0.009856 -0.005544 -0.011659 -0.001729  \n",
       "171  0.006375  0.041935  0.016515  0.060147 -0.009017  \n",
       "\n",
       "[172 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "all_sheets = pd.read_excel('/home/ksrayaldin/FINMATH/hw1data.xlsx', sheet_name=None)\n",
    "print(\"Available sheets:\", list(all_sheets.keys()))\n",
    "\n",
    "# Access individual sheets\n",
    "sheet1 = all_sheets['descriptions']\n",
    "sheet2 = all_sheets['prices']\n",
    "sheet3 = all_sheets['total returns']\n",
    "df = all_sheets['excess returns']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef436bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean excess returns:\n",
      "BWX   -0.007716\n",
      "DBC   -0.005292\n",
      "EEM    0.029339\n",
      "EFA    0.061775\n",
      "HYG    0.041371\n",
      "IEF    0.016404\n",
      "IYR    0.074916\n",
      "PSP    0.092561\n",
      "QAI    0.019327\n",
      "SPY    0.128141\n",
      "TIP    0.020502\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Remove the Date column from the dataframe\n",
    "df_no_date = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# Calculate mean excess returns for each asset\n",
    "mean_returns = df_no_date.mean() * 12\n",
    "print(\"\\nMean excess returns:\")\n",
    "print(mean_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a0829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Annualized volatilities:\n",
      "BWX    0.082789\n",
      "DBC    0.166553\n",
      "EEM    0.176164\n",
      "EFA    0.150903\n",
      "HYG    0.075928\n",
      "IEF    0.063442\n",
      "IYR    0.168675\n",
      "PSP    0.213370\n",
      "QAI    0.049073\n",
      "SPY    0.142839\n",
      "TIP    0.051115\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate covariance matrix\n",
    "cov_matrix = df_no_date.cov() * 12\n",
    "\n",
    "# Calculate volatilities (standard deviations) and annualize them\n",
    "volatilities = (df_no_date.std() * (12 ** 0.5))\n",
    "print(\"\\nAnnualized volatilities:\")\n",
    "print(volatilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e8fb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sharpe ratios:\n",
      "BWX   -0.093202\n",
      "DBC   -0.031774\n",
      "EEM    0.166542\n",
      "EFA    0.409372\n",
      "HYG    0.544873\n",
      "IEF    0.258569\n",
      "IYR    0.444143\n",
      "PSP    0.433804\n",
      "QAI    0.393838\n",
      "SPY    0.897103\n",
      "TIP    0.401091\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sharpe_ratios = mean_returns / volatilities\n",
    "print(\"\\nSharpe ratios:\")\n",
    "print(sharpe_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b28363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation matrix:\n",
      "          BWX       DBC       EEM       EFA       HYG       IEF       IYR  \\\n",
      "BWX  1.000000  0.191116  0.621673  0.602820  0.602555  0.580891  0.552557   \n",
      "DBC  0.191116  1.000000  0.511667  0.500922  0.461887 -0.300207  0.280518   \n",
      "EEM  0.621673  0.511667  1.000000  0.819925  0.691167  0.026704  0.584063   \n",
      "EFA  0.602820  0.500922  0.819925  1.000000  0.787191  0.042639  0.699292   \n",
      "HYG  0.602555  0.461887  0.691167  0.787191  1.000000  0.187258  0.739356   \n",
      "IEF  0.580891 -0.300207  0.026704  0.042639  0.187258  1.000000  0.316532   \n",
      "IYR  0.552557  0.280518  0.584063  0.699292  0.739356  0.316532  1.000000   \n",
      "PSP  0.526692  0.453303  0.750109  0.895320  0.812157  0.022436  0.749836   \n",
      "QAI  0.630276  0.475311  0.774697  0.847864  0.807893  0.179761  0.718529   \n",
      "SPY  0.439994  0.432162  0.687751  0.845863  0.793518  0.000815  0.754711   \n",
      "TIP  0.675151  0.109006  0.378792  0.394821  0.538648  0.754102  0.598742   \n",
      "\n",
      "          PSP       QAI       SPY       TIP  \n",
      "BWX  0.526692  0.630276  0.439994  0.675151  \n",
      "DBC  0.453303  0.475311  0.432162  0.109006  \n",
      "EEM  0.750109  0.774697  0.687751  0.378792  \n",
      "EFA  0.895320  0.847864  0.845863  0.394821  \n",
      "HYG  0.812157  0.807893  0.793518  0.538648  \n",
      "IEF  0.022436  0.179761  0.000815  0.754102  \n",
      "IYR  0.749836  0.718529  0.754711  0.598742  \n",
      "PSP  1.000000  0.873395  0.891687  0.408005  \n",
      "QAI  0.873395  1.000000  0.866845  0.516670  \n",
      "SPY  0.891687  0.866845  1.000000  0.381625  \n",
      "TIP  0.408005  0.516670  0.381625  1.000000  \n",
      "\n",
      "Most correlated pair:\n",
      "PSP and EFA: 0.895\n",
      "\n",
      "Least correlated pair:\n",
      "IEF and SPY: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = df_no_date.corr()\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Find the most and least correlated pairs\n",
    "# Get the lower triangle of correlation matrix excluding diagonal\n",
    "lower_triangle = np.tril(corr_matrix, k=-1)\n",
    "\n",
    "# Find max correlation and corresponding pair\n",
    "max_corr = np.max(lower_triangle)\n",
    "max_corr_idx = np.where(lower_triangle == max_corr)\n",
    "max_pair = (corr_matrix.index[max_corr_idx[0][0]], corr_matrix.columns[max_corr_idx[1][0]])\n",
    "\n",
    "# Absolute value of correlation matrix to find least correlated pairs\n",
    "abs_matrix = abs(corr_matrix)\n",
    "\n",
    "# Find min correlation and corresponding pair\n",
    "min_corr = np.min(abs_matrix)\n",
    "min_corr_idx = np.where(abs_matrix == min_corr)\n",
    "min_pair = (corr_matrix.index[min_corr_idx[0][0]], corr_matrix.columns[min_corr_idx[1][0]])\n",
    "\n",
    "print(\"\\nMost correlated pair:\")\n",
    "print(f\"{max_pair[0]} and {max_pair[1]}: {max_corr:.3f}\")\n",
    "\n",
    "print(\"\\nLeast correlated pair:\")\n",
    "print(f\"{min_pair[0]} and {min_pair[1]}: {min_corr:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c62198f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sharpe ratios ranked from highest to lowest:\n",
      "SPY    0.897103\n",
      "HYG    0.544873\n",
      "IYR    0.444143\n",
      "PSP    0.433804\n",
      "EFA    0.409372\n",
      "TIP    0.401091\n",
      "QAI    0.393838\n",
      "IEF    0.258569\n",
      "EEM    0.166542\n",
      "DBC   -0.031774\n",
      "BWX   -0.093202\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Sort mean returns from highest to lowest\n",
    "sorted_sharpe = sharpe_ratios.sort_values(ascending=False)\n",
    "print(\"\\nSharpe ratios ranked from highest to lowest:\")\n",
    "print(sorted_sharpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98763f5",
   "metadata": {},
   "source": [
    "TIPS has the 6th highest sharpe ratio for excess returns & 7th highest mean returns. It is a roughly average instrument in the portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd64865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tangency Portfolio Weights:\n",
      "          Weight\n",
      "Asset           \n",
      "BWX    -6.117430\n",
      "DBC    -0.112341\n",
      "EEM     0.853591\n",
      "EFA     0.385285\n",
      "HYG     2.634922\n",
      "IEF     9.010591\n",
      "IYR    -2.382146\n",
      "PSP    -1.716842\n",
      "QAI   -13.616007\n",
      "SPY    10.729782\n",
      "TIP     1.330595\n"
     ]
    }
   ],
   "source": [
    "# Calculate tangency portfolio weights\n",
    "mu = mean_returns\n",
    "sigma_inv = np.linalg.inv(cov_matrix)\n",
    "ones = np.ones(mu.shape[0])\n",
    "\n",
    "# Compute weights\n",
    "w_tang = (sigma_inv @ mu) / (ones.T @ sigma_inv @ mu)\n",
    "# Create a DataFrame for better display\n",
    "weights_df = pd.DataFrame({\n",
    "    'Asset': mu.index,\n",
    "    'Weight': w_tang\n",
    "})\n",
    "weights_df = weights_df.set_index('Asset')\n",
    "\n",
    "print(\"Tangency Portfolio Weights:\")\n",
    "print(weights_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d30a1e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equally-weighted portfolio mean:\n",
      "0.01\n",
      "Equally-weighted portfolio volatility:\n",
      "0.022472197876925056\n",
      "Equally-weighted portfolio Sharpe ratio:\n",
      "0.4449943016151624\n"
     ]
    }
   ],
   "source": [
    "# Create equal weights that sum to 1\n",
    "w_e = np.ones(mu.shape[0]) / mu.shape[0]\n",
    "\n",
    "# Scale weights to achieve target mean return of 0.01\n",
    "scaling = 0.01 / (w_e @ mu)\n",
    "w_e = scaling * w_e\n",
    "\n",
    "# Calculate portfolio statistics\n",
    "port_mean = w_e @ mu\n",
    "port_vol = np.sqrt(w_e @ cov_matrix @ w_e)\n",
    "port_sharpe = port_mean / port_vol\n",
    "\n",
    "print(\"Equally-weighted portfolio mean:\")\n",
    "print(port_mean)\n",
    "\n",
    "print(\"Equally-weighted portfolio volatility:\") \n",
    "print(port_vol)\n",
    "\n",
    "print(\"Equally-weighted portfolio Sharpe ratio:\")\n",
    "print(port_sharpe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5cc643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Risk Parity Portfolio Weights:\n",
      "         Weight\n",
      "Asset          \n",
      "BWX    0.036102\n",
      "DBC    0.008920\n",
      "EEM    0.007973\n",
      "EFA    0.010866\n",
      "HYG    0.042922\n",
      "IEF    0.061480\n",
      "IYR    0.008697\n",
      "PSP    0.005435\n",
      "QAI    0.102755\n",
      "SPY    0.012128\n",
      "TIP    0.094707\n",
      "\n",
      "Risk Parity portfolio mean:\n",
      "0.01\n",
      "\n",
      "Risk Parity portfolio volatility:\n",
      "0.021955401079611393\n",
      "\n",
      "Risk Parity portfolio Sharpe ratio:\n",
      "0.455468791653566\n"
     ]
    }
   ],
   "source": [
    "# Calculate risk-parity portfolio weights\n",
    "# Initialize weights equally\n",
    "w_rp = np.ones(mu.shape[0]) / mu.shape[0]\n",
    "\n",
    "# Calculate individual variances from the covariance matrix\n",
    "variances = np.diag(cov_matrix)\n",
    "\n",
    "# Calculate weights as 1/variance\n",
    "w_rp = 1/variances\n",
    "\n",
    "# Normalize weights to sum to 1\n",
    "w_rp = w_rp/np.sum(w_rp)\n",
    "\n",
    "# Scale weights to achieve target mean return of 0.01\n",
    "scaling = 0.01 / (w_rp @ mu)\n",
    "w_rp = scaling * w_rp\n",
    "\n",
    "# Create DataFrame for risk parity weights\n",
    "rp_weights_df = pd.DataFrame({\n",
    "    'Asset': mu.index,\n",
    "    'Weight': w_rp\n",
    "})\n",
    "rp_weights_df = rp_weights_df.set_index('Asset')\n",
    "\n",
    "print(\"\\nRisk Parity Portfolio Weights:\")\n",
    "print(rp_weights_df)\n",
    "\n",
    "# Calculate portfolio statistics\n",
    "port_mean_rp = w_rp @ mu\n",
    "port_vol_rp = np.sqrt(w_rp @ cov_matrix @ w_rp)\n",
    "port_sharpe_rp = port_mean_rp / port_vol_rp\n",
    "\n",
    "print(\"\\nRisk Parity portfolio mean:\")\n",
    "print(port_mean_rp)\n",
    "print(\"\\nRisk Parity portfolio volatility:\")\n",
    "print(port_vol_rp)\n",
    "print(\"\\nRisk Parity portfolio Sharpe ratio:\")\n",
    "print(port_sharpe_rp)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c926397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum Variance Portfolio Weights:\n",
      "         Weight\n",
      "Asset          \n",
      "BWX   -0.748501\n",
      "DBC   -0.085829\n",
      "EEM   -0.445357\n",
      "EFA    0.265846\n",
      "HYG    0.887746\n",
      "IEF   -0.309273\n",
      "IYR   -0.282977\n",
      "PSP   -0.945990\n",
      "QAI    8.035098\n",
      "SPY   -0.870762\n",
      "TIP    1.649108\n",
      "\n",
      "Minimum Variance portfolio mean:\n",
      "0.009999999999999988\n",
      "\n",
      "Minimum Variance portfolio volatility:\n",
      "0.18519687924516184\n",
      "\n",
      "Minimum Variance portfolio Sharpe ratio:\n",
      "0.0539965902274308\n"
     ]
    }
   ],
   "source": [
    "#Calculate MV portfolio weights\n",
    "# Calculate minimum variance portfolio weights\n",
    "w_mv = sigma_inv @ ones / (ones.T @ sigma_inv @ ones)\n",
    "\n",
    "# Scale weights to achieve target mean return of 0.01\n",
    "scaling = 0.01 / (w_mv @ mu)\n",
    "w_mv = scaling * w_mv\n",
    "\n",
    "# Create DataFrame for minimum variance portfolio weights\n",
    "mv_weights_df = pd.DataFrame({\n",
    "    'Asset': mu.index,\n",
    "    'Weight': w_mv\n",
    "})\n",
    "mv_weights_df = mv_weights_df.set_index('Asset')\n",
    "\n",
    "print(\"\\nMinimum Variance Portfolio Weights:\")\n",
    "print(mv_weights_df)\n",
    "\n",
    "# Calculate portfolio statistics\n",
    "port_mean_mv = w_mv @ mu\n",
    "port_vol_mv = np.sqrt(w_mv @ cov_matrix @ w_mv)\n",
    "port_sharpe_mv = port_mean_mv / port_vol_mv\n",
    "\n",
    "print(\"\\nMinimum Variance portfolio mean:\")\n",
    "print(port_mean_mv)\n",
    "print(\"\\nMinimum Variance portfolio volatility:\")\n",
    "print(port_vol_mv)\n",
    "print(\"\\nMinimum Variance portfolio Sharpe ratio:\")\n",
    "print(port_sharpe_mv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371965c",
   "metadata": {},
   "source": [
    "To summarize the three allocations: All have roughly a mean excess return of .01. Therefore, the portfolio with the highest Sharpe Ratio & lowest variance will be the best performing one. The MV portfolio as such performs the best, while the other two perform slightly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea828b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8443d",
   "metadata": {},
   "source": [
    "# 4. EXTRA: Out-of-Sample Performance\n",
    "\n",
    "<i>This section is not graded, and you do not need to submit it. Still, we may discuss it in class, in which case, you would be expected to know it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4b25b",
   "metadata": {},
   "source": [
    "### 1. One-step Out-of-Sample (OOS) Performance\n",
    "Let’s divide the sample to both compute a portfolio and then check its performance out of sample.\n",
    "* Using only data through the end of `2022`, compute the weights built in Section 3.\n",
    "* Rescale the weights, (using just the in-sample data,) to set each allocation to have the same mean return of $\\mutarg$.\n",
    "* Using those weights, calculate the portfolio’s Sharpe ratio within that sample.\n",
    "* Again using those weights, (derived using data through `2022`,) calculate the portfolio’s OOS Sharpe ratio, which is based only on performance in `2023-2024`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281c023",
   "metadata": {},
   "source": [
    "### 2. Rolling OOS Performance\n",
    "\n",
    "Iterate the Out-of-Sample performance every year, not just the final year. Namely,\n",
    "* Start at the end of `2015`, and calculate the weights through that time. Rescale them using the mean returns through that time.\n",
    "* Apply the weights to the returns in the upcoming year, (`2016`.)\n",
    "* Step forward a year in time, and recompute.\n",
    "* Continue until again calculating the weights through `2023` and applying them to the returns in `2024`.\n",
    "\n",
    "Report the mean, volatility, and Sharpe from this dynamic approach for the following portfolios:\n",
    "* mean-variance (tangency)\n",
    "* equally-weighted\n",
    "* risk-parity\n",
    "* regularized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc96f3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b0cf7",
   "metadata": {},
   "source": [
    "# 5. EXTRA: Without a Riskless Asset\n",
    "\n",
    "<i>This section is not graded, and you do not need to submit it. Still, we may discuss it in class, in which case, you would be expected to know it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1ff49",
   "metadata": {},
   "source": [
    "Re-do Section 2 above, but in the model without a risk-free rate.\n",
    "\n",
    "That is, build the MV allocation using the two-part formula in the `Mean-Variance` section of the notes.\n",
    "* This essentially substitutes the risk-free rate with the minimum-variance portfolio.\n",
    "* Now, the allocation depends nonlinearly on the target mean return, $\\mutarg$. (With a risk-free rate, we simply scale the weights up and down to achieve the mean return.)\n",
    "\n",
    "You will find that, conceptually, the answers are very similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5734f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c864754",
   "metadata": {},
   "source": [
    "# 6. EXTRA: Bayesian Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f110c351",
   "metadata": {},
   "source": [
    "Try the following allocation among the choices in `Section 3`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dae4fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c70bb005",
   "metadata": {},
   "source": [
    "\n",
    "#### Regularized (REG)\n",
    "Much like the Mean-Variance portfolio, set the weights proportional to \n",
    "$$\\wREG \\sim \\widehat{\\Sigma}^{-1}\\mux$$\n",
    "but this time, use a regularized covariance matrix,\n",
    "$$\\widehat{\\Sigma} = \\frac{\\Sigma + \\Sigma_D}{2}$$\n",
    "where $\\Sigma_D$ denotes a *diagonal* matrix of the security variances, with zeros in the off-diagonals.\n",
    "\n",
    "Thus, $\\widehat{\\Sigma}$ is obtained from the usual covariance matrix, $\\Sigma$, but shrinking all the covariances to half their estimated values. \n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
